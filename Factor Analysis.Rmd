---
title: "Factor Analysis"
author: "Neehaar Gandhi, David Liu, Josh Brooks"
date: "4/21/2019"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(psych)
library(rela)
library(corrplot)
library(PerformanceAnalytics)
```

```{r}
#read data
PD <- read.csv("data.csv", header=TRUE, stringsAsFactors = FALSE)

#isolate only complete cases
PD <- PD[complete.cases(PD),]

#choose a subset of the data that makes sense  

#data <- PD[,c(4,6,9,16,17,18,19,29,30,31)]
#data[,5] <- as.numeric(data[,5])

#the above was our first attempt to choose good indicators. the second attempt, which is below, seemed more successful. 
data <- PD[,c(29,30,31,32,33,37,38,39,40,47,49,55,56,57)]
head(data)
```

1. Look through indicators (questions). Think about which indicators might be related through latent factors.
*We chose the following skill 14 rating variables as indicators. Among this list are a number of traits that we suspect to be related to offensive position success, as well as a number of traits which we suspect to be related to defensive position success.*  


2. Compute the correlation matrix between all indicators (you may want to do this in batches). Comment on relationships you do/do not observe.
```{r}
data1 <- data[,1:length(data)]
cor(data1)
corrplot(cor(data1),method = "ellipse", order="hclust", tl.col = "black", tl.cex=.8)
chart.Correlation(data1, histogram=TRUE, pch=19)
```
*Our correlation matrix and its visual aids show that we have a number of indicators that are correlated deeply, suggesting the viability of finding some underlying factors affecting variation. Some correlation magnitudes are smaller, but many are somewhat large, between 0.6 and 0.9.*  


3. Compute KMO or other measure (i.e. just look at matrix produced above) to comment on suitability of data for factor analysis.
```{r}
datamat <- data.matrix(data1)
output1 <- paf(datamat, eigcrit=1, convcrit=.001)
print(output1$KMO)
summary(output1)
```

*The selected indicators give us a Kaiser-Meyer-Olkin output of 0.83571, which is well above the acceptable range, and verging on good -- "meritorious", according to Kaiser's rating scale. Generally speaking, this means that there is a relatively large degree of common variance among our variables, and thus our data are adequate enough for facter analysis.*  

*We initially tried a different and less judiciously chosen set of predictors, which gave a KMO value of about 0.65. This number, while also probably acceptable for factor analysis, is on the low side. As such, we decided to choose a better sample. The indicators we chose previously were columns 4, 6, 9, 16, 17, 18, 19, 29, 30, 31.*  


4. Use Principle Components (or appropriate option in Factor Analysis) to decide on a number of latent factors. You can use Scree Plot, eigenvalue>1, or parallel analysis. 
```{r}
#principal components analysis
#chose to do PCA on the covariance matrix because the variable scales are similar (they are 0-100)
fit <- princomp(data, cor=FALSE)
summary(fit)
loadings(fit)

#eigenvalue > 1
print(summary(fit),digits=2,loadings=fit$loadings,cutoff=0)
#all 14 components have a stdev and variance (eigenvalue) of greater than one, perhaps use a different metric to determine number of latent factors. 

#scree plot
plot(fit, type="lines", main="Scree Plot of FIFA Data",col="red",lwd=2,pch=19,cex=1.2)
#seems to be an elbow at the 4th or 5th component
```

*We ran PCA using the covariance matrix because the scales of the variables were identical; all are ratings between 0 and 100. The PCA of our data, as most effectively represented by the scree plot, suggests that either three or four components would be the best choices. Thus, we will aim to use four latent factors in our factor analysis. The eigenvalues for all 14 components (indicators) are all larger than 1, so they do not prove a very useful tool for choosing a number of factors.*  


5. Perform a series of factor analysies using orthogonal models. First, try at least two extraction methods (choose from Principle Components, Principle Axis Factoring, Iterative Principle Components, Maximum Likelihood), Use some method for comparing extraction methods to choose a 'best' method (i.e. RMSR or # residuals greater than .05).


#### Perform Factor Analysis using iterative PCA
```{r}
fact_pca <- fa(data, nfactors=4, rotation="none", SMC=FALSE, fm="pa")
fact_pca
```

Loading plot for first two factors.
```{r}
plot(fact_pca$loadings, pch=18, col='red')
abline(h=0)
abline(v=0)
text(fact_pca$loadings, labels=names(data),cex=0.8)
```

Correlation Matrix Analyses
```{r}
#get reproduced correlation matrix
repro3 <- fact_pca$loadings%*%t(fact_pca$loadings)
#residual correlation matrix
resid3 <- cor(data)-repro3
round(resid3,2)

#get root-mean squared residuals - again, in output above
len <- length(resid3[upper.tri(resid3)])
RMSR3 <- sqrt(sum(resid3[upper.tri(resid3)]^2)/len)
RMSR3

#get proportion of residuals greater than 0.05 in absolute value
sum(rep(1,len)[abs(resid3[upper.tri(resid3)])>0.05])/len

```


#### Factor Analysis using Maximum Likelihood
```{r}
fact_ml <- fa(data, nfactors=4, rotation="none", fm="ml")
fact_ml
```

Loading plot for first two factors
```{r}
plot(fact_ml$loadings, pch=18, col='red')
abline(h=0)
abline(v=0)
text(fact_ml$loadings, labels=names(data),cex=0.8)
```

Correlation matrix analysis
```{r}
#get reproduced correlation matrix
repro_ml <- fact_ml$loadings%*%t(fact_ml$loadings)
#residual correlation matrix
resid_ml <- cor(data)-repro_ml
round(resid_ml,2)

#get root-mean squared residuals - already provided in output actually
len <- length(resid_ml[upper.tri(resid_ml)])
RMSR_ml <- sqrt(sum(resid_ml[upper.tri(resid_ml)]^2)/len)
RMSR_ml

#get proportion of residuals greater than 0.05 in absolute value
sum(rep(1,len)[abs(resid_ml[upper.tri(resid_ml)])>0.05])/len

```


#### Factor Analysis using PAF
```{r}
#this uses the fa() function in the psych package.  Note that this fails with only 2 factors
fact_paf <- fa(data, nfactors=4, rotation="none", fm="pa")
fact_paf
```

Loading plot for first two factors
```{r}
plot(fact_paf$loadings, pch=18, col='red')
abline(h=0)
abline(v=0)
text(fact_paf$loadings, labels=names(data),cex=0.8)
```

Correlation matrix analysis
```{r}
#get reproduced correlation matrix
repro_paf <- fact_paf$loadings%*%t(fact_paf$loadings)
#residual correlation matrix
resid_paf <- cor(data)-repro_paf
round(resid_paf,2)
#get root-mean squared residuals - already provided in output actually
len <- length(resid_paf[upper.tri(resid_paf)])
RMSR_paf <- sqrt(sum(resid_paf[upper.tri(resid_paf)]^2)/len)
RMSR_paf
#get proportion of residuals greater than 0.05 in absolute value
sum(rep(1,len)[abs(resid_paf[upper.tri(resid_paf)])>0.05])/len
```
*The PAF and iterative PCA methods produced similar results whereas maximum likelihood produced a different outcome. We would expect these PAF and iterative PCA to produce similar results since they all fall within common factor analysis strategy and iterative PCA and PAF are based off similar iterative techniques. Looking at the plots between maximum likelihood and iterative PCA, there are clear differences in the common factors; however, the distances between many indicator variables remain roughly the same. For instance, indicator variables that we would associate with defense tend to remain far away from indicator variables that are more tied with offense. *

*Since the maximum likelihood extraction method produced the lowest RMSR value and also has the smallest proportion of residuals greater than .05, we will work with this method going forward.*

6. Once you've chosen an extraction method, try a varimax and/or a quartimax rotation. Pick one of these rotations and discuss the interpretation of the final factors. Make one or more loading plots as appropriate.

#### Factor Analysis using maximum likelihood with Varimax Rotation
*Note: this was the best extraction method*
```{r}
fact_mlRot <- fa(data, nfactors=4, rotation="varimax", fm="ml")
fact_mlRot
```

Loading plot for first two factors
```{r}
plot(fact_mlRot$loadings, pch=18, col='red')
abline(h=0)
abline(v=0)
text(fact_mlRot$loadings, labels=names(data),cex=0.8)
```
Loading plot for last two factors
```{r}
plot(fact_mlRot$loadings[,3:4], pch=18, col='red')
abline(h=0)
abline(v=0)
text(fact_mlRot$loadings[,3:4], labels=names(data),cex=0.8)
```

Correlation matrix analysis
```{r}
#get reproduced correlation matrix
repro_mlRot <- fact_mlRot$loadings%*%t(fact_mlRot$loadings)
#residual correlation matrix
resid_mlRot <- cor(data)-repro_mlRot
round(resid_mlRot,2)

#get root-mean squared residuals - already provided in output actually
len <- length(resid_mlRot[upper.tri(resid_mlRot)])
RMSR_mlRot <- sqrt(sum(resid_mlRot[upper.tri(resid_mlRot)]^2)/len)
RMSR_mlRot

#get proportion of residuals greater than 0.05 in absolute value
sum(rep(1,len)[abs(resid_mlRot[upper.tri(resid_mlRot)])>0.05])/len

```

*The first latent factor seems to be about defensive versus offensive abilities.
The second latent factor has large loadings for midfield player skills, which we might think of as the median skills that do not demand extremes (heading, strength, tackling, marking).
The third latent factor expresses variation on skills that are not related to athleticism — there are positive values for passing and finishing, but negative values for running and tackling. We can think of this factor as related to a player’s dependence on finesse. 
The fourth latent factor explains variation based on how physical a player’s play style is.* 