---
title: "SDS 363 - Final Project"
author: "Neehaar Gandhi, David Liu, Josh Brooks"
date: "4/29/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Introduction
This project analyzes the data that drives the 2018 edition of the officially licensed Fédération Internationale de Football Association (International Federation of Association Football) video game, titled FIFA 18, produced by Electronic Arts (EA). Association football (soccer) is by far the most popular sport in the world, so it should come as no surprise that the game has became something of a cultural icon. To date, the FIFA franchise is the sixth-best selling video game series in history, with its popularity likely to continue rising. A new version of the game is released once per year, making the 2018 edition of the game the twenty-fifth installment. 

Within the electronic entertainment industry, sports video games present a unique, data-driven challenge. Gamers want to be titillated and drawn in, while rapid sports fans simply will never tolerate misrepresentation of their favorite players. This tension -- between manufacturing a maximally effective fictitious world and satisfying players’ wish to play with real people -- leads to a wealth of data-driven challenges. This is the central consideration that underscores our analysis. We will use the game’s player rating data, which is described in detail in the following section, to answer questions about the relationships between soccer player attributes.

#Design and Primary Questions
One area of interest for us is understanding better how EA producers ultimately come to their “Overall” player ratings, which is often markedly higher than the average of their other skill categories. This leads us to believe that there is an element of subjectivity in determining overall player scores; EA sports determines that some players have the x-factor needed to boost their overall scores, while others do not. Thus, a principal question for us is: *What sets of variables correspond with higher overall scores, and can we use those variables to tell a story about what an ‘x-factor’ might be?* To answer this we will use factor analysis. In so doing we hope we will be able to find a latent factor that might tell a story about what makes players valuable on the field. 

Further, we seek to look for patterns of players within particular position groups. *Within broad positional categories -- back, midfielder, forward -- can we see discernable subgroups of players?* Again, we would certainly expect to see this, given subjective understandings of soccer tactics. Using k-means as a clustering technique, we hope to create a readable picture of positional subgroups.

Finally, we want to look at the interactions between FIFA players’ nationalities and their skills in various areas. In the international soccer community, it is very common to talk about geographic regions as very different. For instance, it has become accepted wisdom to say that Latin American teams play a more skillful, less team-oriented style, while European teams play with more precision and less individual skill. We hope to see whether the data bear this out. In this vein, our third question for analysis will be: *Do different country groups create players that are differently abled in particular facets of the game?* To test this we will use MANOVA and attempt to establish differences in ability means between various nationality groups.

#Data
```{r, include=FALSE}
#include relevant libraries for project
library(aplpack)
library(fpc)
library(cluster)
library(ape)
library(ggplot2)
library(DBItest)
library(tidyverse)  
library(factoextra)
library(psych)
library(rela)
library(corrplot)
library(PerformanceAnalytics)

PD <- read.csv("data.csv", header=TRUE, stringsAsFactors = FALSE)
#isolate only complete cases
PD <- PD[complete.cases(PD),]

#make sure data is numeric where it ought to be numeric (in the original dataset, for some reason some player heights/weights/etc [variables that need to be put in numeric terms], were read as characters [perhaps due to spaces or apostrophes])
PD[,4] <- as.numeric(PD[,4])
PD[,6] <- as.numeric(PD[,6])
PD[,7] <- as.numeric(PD[,7])
PD[,9] <- as.numeric(PD[,9])
PD[,17] <- as.numeric(PD[,17])
PD[,18] <- as.numeric(PD[,18])
```
The dataset is quite large, as a result of the game’s intention of accurately representing every male professional soccer player in the world. While the video game surely requires an even larger ocean of data to operate, we will concern ourselves only with the player rating data -- those data that describe the qualities of individual athletes and their particular traits and skills. 
There are a few broad categories of data in the set. The first type of variable is made up of the information that describes a player and his location. These include physical traits like height and weight, as well as the club for which the player plays professionally, the country for which the player plays in international competition, the player’s wage per game-week, and the like. All of this information is taken directly from publicly available postings by the players’ clubs. Wage is rounded to the nearest thousand, which could change the data somewhat at lower numbers. 

The second, and ultimately the most significant, type of variable in the set comprises ratings for various in-game skills and actions on a 100 point scale. The full list has more than thirty predictors, which will be detailed below. Among them are skilled related to playing offense, playing defense, passing, athleticism, and everyone else that one must do on a soccer field. Some examples are: “Ball Control” (ability to keep possession of the ball and place it in a favorable position for future actions), “Finishing” (generalized fast-twitch ability to accurately kick the ball into the goal after receiving a pass), “Marking” (ability to successfully keep with a player while playing defense, and prevent him from receiving the ball), and “Agility” (a general measure of ability to move quickly over short distances and control one’s body through space). 

These skill ratings are, of course, highly subjective. They do not appear out of thin air; they must be created and compiled by human raters. EA are upfront about their process. They delegate the much of the task of watching and evaluating to a network of over 9000 “data reviewers”, made up of professionals (coaches) and amateurs (qualified fans). The material from the reviewers comes to a group of a few-hundred editors, who compile the data into the rating categories listed below. With this in mind, we can think of these ratings as the results of a large survey, in which soccer fanatics are asked to watch a great deal of game film and give answers to questions about player ability. This is not exactly true -- EA’s producers and editors tweak the data as they see fit, and the sampling of data reviewers is surely not perfectly random -- but thinking of it this way provides a general picture.

The third group of predictors is smaller and generally less useful. It is made up of semi-categorical predictors (categorical skill ratings with five levels). These include “Skill Moves” and “International Reputation”. We will largely ignore these in our analysis -- they are less about soccer, and more about game mechanics. Skill moves is a good example of this category. From 1 to 5, as players go to up higher thresholds of skill rating, they gain the ability to carry out particular in-game actions.

#Descriptive plots, summary statistics
```{r}

```
[insert analysis here]

```{r}
summary(PD)
head(PD)

# Frequency Examinations for Categoricals
table(PD$Nationality)
table(PD$Club)

# Plot to give a sense of the variance in number of players produced by different countries
plot(table(PD$Nationality))

# boxplots for select variables
boxplotCols <- c(4, 6, 7, 9, 11, 13, 17:57)
# boxplotCols <- c(4, 6, 8) # use this to get a sense of plots when u don't want to load all plots
PDNames <- colnames(PD)
for (col in boxplotCols){
  boxplot(PD[, col], main=PDNames[col])
}

# histograms
for (col in boxplotCols){
  hist(PD[, col], main=PDNames[col])
}

# Correlation matrix shown in latter part
```

#Multivariate Analysis and Discussion of Results

*Factor Analysis*
In conducting factor analysis for our data, we first had to determine the suitability by computing KMO (Kaiser-Meyer-Olkin). Further, a test such as Principal Component Analysis (PCA) must be conducted to determine a number of latent factors that we are looking for, and as such, we will compute PCA on the covariance matrix because the variable scales we utilized are similar (ranging from 0-100). 
```{r}
#subset data that we will use for factor analysis
data <- PD[,c(4, 6, 7, 9, 11, 12, 13, 16:57)]
```

```{r}
#partition into thirds so that correlation plots are more readible
whole <- data[,1:length(data)]
data1 <- data[,1:length(data)/3]
data2 <- data[,(length(data)/3):(2*(length(data)/3))]
data3 <- data[,(2*(length(data)/3)):length(data)]

#compute the correlation matrix between all indicators
corrplot(cor(data1),method = "ellipse", order="hclust", tl.col = "black", tl.cex=.8)
corrplot(cor(data2),method = "ellipse", order="hclust", tl.col = "black", tl.cex=.8)
corrplot(cor(data3),method = "ellipse", order="hclust", tl.col = "black", tl.cex=.8)

#chart.Correlation(data1, histogram=TRUE, pch=19)
#chart.Correlation(data2, histogram=TRUE, pch=19)
#chart.Correlation(data3, histogram=TRUE, pch=19)
```
[comment on relationships that we do/don't observe]

```{r}
#KMO
datamat <- data.matrix(whole)
output1 <- paf(datamat, eigcrit=1, convcrit=.001)
print(output1$KMO)
summary(output1)

#Using PCA
fit <- princomp(whole, cor=FALSE)
summary(fit)
loadings(fit)

#eigenvalue > 1
print(summary(fit),digits=2,loadings=fit$loadings,cutoff=0)
#components 1-35 greater than one, perhaps use a different metric to determine number of latent factors, because this seems to be too many. 

#scree plot
plot(fit, type="lines", main="Scree Plot of FIFA Data",col="red",lwd=2,pch=19,cex=1.2)
#elbow at 3rd component
```

Below is the actual Factor Analysis Test
```{r}
#had to increase the lower bound from default 0.005 in order to allow the solution to converge 
fact_ml <- factanal(data, factors=3, rotation="varimax", fm="ml", lower = 0.02)
fact_ml
```

Loading plot for first two factors
```{r}
plot(fact_ml$loadings, pch=18, col='red')
abline(h=0)
abline(v=0)
text(fact_ml$loadings, labels=names(whole),cex=0.8)
```

Correlation matrix analysis
```{r}
#get reproduced correlation matrix
repro_ml <- fact_ml$loadings%*%t(fact_ml$loadings)
#residual correlation matrix
resid_ml <- cor(data)-repro_ml
round(resid_ml,2)

#get root-mean squared residuals - already provided in output actually
len <- length(resid_ml[upper.tri(resid_ml)])
RMSR_ml <- sqrt(sum(resid_ml[upper.tri(resid_ml)]^2)/len)
RMSR_ml

#get proportion of residuals greater than 0.05 in absolute value
sum(rep(1,len)[abs(resid_ml[upper.tri(resid_ml)])>0.05])/len
```
[insert analysis here]


*K-Means*
```{r}
#make a new matrix for K-Mean Analysis
KPD <- PD

KPD[,c("X", "ID", "Name", "Nationality", "Club", "PreferredFoot", "BodyType", "Position", "InternationalReputatiton", "WeakFoot", "SkillMoves")] <- list(NULL)

KPD
KPD[,7] <- as.numeric(KPD[,7])
#sapply(1:ncol(PD), function(a) {class(PD[,a])})

KPD <- scale(KPD)
```

```{r}
#read data
PD <- read.csv("data.csv", header=TRUE, stringsAsFactors = FALSE)

PD <- PD[complete.cases(PD),]

rowNames <- PD[,"Name"]

PD2 <- PD
rowNames <- PD2[,"Name"]

PD[,c("X", "ID", "Name", "Nationality", "Club", "PreferredFoot", "BodyType", "Position", "InternationalReputatiton", "WeakFoot", "SkillMoves")] <- list(NULL)

PD[,7] <- as.numeric(PD[,7])

#sapply(1:ncol(PD), function(a) {class(PD[,a])})

SPD <- scale(PD)

SPD



#get defensive subroup
PDdef <- PD2[ which(PD2$Position == "CB" | PD2$Position == "RB" | PD2$Position == "LB" | PD2$Position == "LCB" | PD2$Position == "RCB"), ]

PDdef[,c("X", "ID", "Name", "Nationality", "Club", "PreferredFoot", "BodyType", "Position", "InternationalReputation", "WeakFoot", "SkillMoves")] <- list(NULL)

PDdef[,6] <- as.numeric(PDdef[,6])

SPDdef <- scale (PDdef)

SPDdef

#get midfield subgroup
PDmid <- PD2[ which(PD2$Position == "CAM" | PD2$Position == "CDM" | PD2$Position == "CM" | PD2$Position == "LAM" | PD2$Position == "RAM" | PD2$Position == "LCM" | PD2$Position == "RCM" | PD2$Position == "LM" | PD2$Position == "RM"), ]

PDmid[,c("X", "ID", "Name", "Nationality", "Club", "PreferredFoot", "BodyType", "Position", "InternationalReputation", "WeakFoot", "SkillMoves")] <- list(NULL)

PDmid[,6] <- as.numeric(PDmid[,6])

SPDmid <- scale (PDmid)

SPDmid

#get offensive subgroup
PDoff <- PD2[ which(PD2$Position == "LS" | PD2$Position == "RS" | PD2$Position == "ST" | PD2$Position == "LW" | PD2$Position == "RW" | PD2$Position == "CF" | PD2$Position == "LF" | PD2$Position == "RF"), ]

PDoff[,c("X", "ID", "Name", "Nationality", "Club", "PreferredFoot", "BodyType", "Position", "InternationalReputation", "WeakFoot", "SkillMoves")] <- list(NULL)

PDoff[,6] <- as.numeric(PDoff[,6])

SPDoff <- scale (PDoff)

SPDoff
```

#### run k-means with all players
```{r}
km1 <- kmeans(SPD,centers=5)

for (i in 1:5){
  print(paste("Players in Cluster",i))
  print(rowNames[km1$cluster==i])
  print (" ")
}

kdata <- SPD
n.lev <- 15

# Calculate the within groups sum of squared error (SSE) for the number of cluster solutions selected by the user
wss <- rnorm(10)
while (prod(wss==sort(wss,decreasing=T))==0) {
  wss <- (nrow(kdata)-1)*sum(apply(kdata,2,var))
  for (i in 2:n.lev) wss[i] <- sum(kmeans(kdata, centers=i)$withinss)}

# Calculate the within groups SSE for 250 randomized data sets (based on the original input data)
k.rand <- function(x){
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

rand.mat <- matrix(0,n.lev,250)

k.1 <- function(x) { 
  for (i in 1:3) {
    r.mat <- as.matrix(suppressWarnings(k.rand(kdata)))
    rand.mat[,i] <- r.mat}
  return(rand.mat)
}

# Same function as above for data with < 3 column variables
k.2.rand <- function(x){
  rand.mat <- matrix(0,n.lev,250)
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

k.2 <- function(x){
  for (i in 1:250) {
    r.1 <- k.2.rand(kdata)
    rand.mat[,i] <- r.1}
  return(rand.mat)
}

# Determine if the data data table has > or < 3 variables and call appropriate function above
if (dim(kdata)[2] == 2) { rand.mat <- k.2(kdata) } else { rand.mat <- k.1(kdata) }

# Plot within groups SSE against all tested cluster solutions for actual and randomized data - 1st: Log scale, 2nd: Normal scale

xrange <- range(1:n.lev)
yrange <- range(log(rand.mat),log(wss))
plot(xrange,yrange, type='n', xlab='Cluster Solution', ylab='Log of Within Group SSE', main='Cluster Solutions against Log of SSE')
for (i in 1:250) lines(log(rand.mat[,i]),type='l',col='red')
lines(log(wss), type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

yrange <- range(rand.mat,wss)
plot(xrange,yrange, type='n', xlab="Cluster Solution", ylab="Within Groups SSE", main="Cluster Solutions against SSE")
for (i in 1:250) lines(rand.mat[,i],type='l',col='red')
lines(1:n.lev, wss, type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

# Calculate the mean and standard deviation of difference between SSE of actual data and SSE of 250 randomized datasets
r.sse <- matrix(0,dim(rand.mat)[1],dim(rand.mat)[2])
wss.1 <- as.matrix(wss)
for (i in 1:dim(r.sse)[2]) {
  r.temp <- abs(rand.mat[,i]-wss.1[,1])
  r.sse[,i] <- r.temp}
r.sse.m <- apply(r.sse,1,mean)
r.sse.sd <- apply(r.sse,1,sd)
r.sse.plus <- r.sse.m + r.sse.sd
r.sse.min <- r.sse.m - r.sse.sd

# Plot differeince between actual SSE mean SSE from 250 randomized datasets - 1st: Log scale, 2nd: Normal scale 

xrange <- range(1:n.lev)
yrange <- range(log(r.sse.plus),log(r.sse.min))
plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='Log of SSE - Random SSE', main='Cluster Solustions against (Log of SSE - Random SSE)')
lines(log(r.sse.m), type="b", col='blue')
lines(log(r.sse.plus), type='l', col='red')
lines(log(r.sse.min), type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), lty=1)

xrange <- range(1:n.lev)
yrange <- range(r.sse.plus,r.sse.min)
plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='SSE - Random SSE', main='Cluster Solutions against (SSE - Random SSE)')
lines(r.sse.m, type="b", col='blue')
lines(r.sse.plus, type='l', col='red')
lines(r.sse.min, type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), lty=1)

# Ask for user input - Select the appropriate number of clusters
#choose.clust <- function(){readline("What clustering solution would you like to use? ")} 
#clust.level <- as.integer(choose.clust())
clust.level <- 5

# Apply K-means cluster solutions - append clusters to CSV file
fit <- kmeans(kdata, clust.level)
aggregate(kdata, by=list(fit$cluster), FUN=mean)
clust.out <- fit$cluster
kclust <- as.matrix(clust.out)
kclust.out <- cbind(kclust, PD)
write.table(kclust.out, file="kmeans_out.csv", sep=",")

# Display Principal Components plot of data with clusters identified

clusplot(kdata, fit$cluster, shade=F, labels=2, lines=0, color=T, lty=4, cex.txt = .3, main='Principal Components plot showing K-means clusters')

```


#### run k-means with defensive players
```{r}
km2 <- kmeans(SPDdef,centers=3)

for (i in 1:5){
  print(paste("Players in Cluster",i))
  print(rowNames[km1$cluster==i])
  print (" ")
}

kdata <- SPDdef
n.lev <- 8

# Calculate the within groups sum of squared error (SSE) for the number of cluster solutions selected by the user
wss <- rnorm(10)
while (prod(wss==sort(wss,decreasing=T))==0) {
  wss <- (nrow(kdata)-1)*sum(apply(kdata,2,var))
  for (i in 2:n.lev) wss[i] <- sum(kmeans(kdata, centers=i)$withinss)}

# Calculate the within groups SSE for 250 randomized data sets (based on the original input data)
k.rand <- function(x){
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

rand.mat <- matrix(0,n.lev,250)

k.1 <- function(x) { 
  for (i in 1:250) {
    r.mat <- as.matrix(suppressWarnings(k.rand(kdata)))
    rand.mat[,i] <- r.mat}
  return(rand.mat)
}

# Same function as above for data with < 3 column variables
k.2.rand <- function(x){
  rand.mat <- matrix(0,n.lev,250)
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

k.2 <- function(x){
  for (i in 1:250) {
    r.1 <- k.2.rand(kdata)
    rand.mat[,i] <- r.1}
  return(rand.mat)
}

# Determine if the data data table has > or < 3 variables and call appropriate function above
if (dim(kdata)[2] == 2) { rand.mat <- k.2(kdata) } else { rand.mat <- k.1(kdata) }

# Plot within groups SSE against all tested cluster solutions for actual and randomized data - 1st: Log scale, 2nd: Normal scale

xrange <- range(1:n.lev)
yrange <- range(log(rand.mat),log(wss))
plot(xrange,yrange, type='n', xlab='Cluster Solution', ylab='Log of Within Group SSE', main='Cluster Solutions against Log of SSE')
for (i in 1:250) lines(log(rand.mat[,i]),type='l',col='red')
lines(log(wss), type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

yrange <- range(rand.mat,wss)
plot(xrange,yrange, type='n', xlab="Cluster Solution", ylab="Within Groups SSE", main="Cluster Solutions against SSE")
for (i in 1:250) lines(rand.mat[,i],type='l',col='red')
lines(1:n.lev, wss, type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

# Calculate the mean and standard deviation of difference between SSE of actual data and SSE of 250 randomized datasets
r.sse <- matrix(0,dim(rand.mat)[1],dim(rand.mat)[2])
wss.1 <- as.matrix(wss)
for (i in 1:dim(r.sse)[2]) {
  r.temp <- abs(rand.mat[,i]-wss.1[,1])
  r.sse[,i] <- r.temp}
r.sse.m <- apply(r.sse,1,mean)
r.sse.sd <- apply(r.sse,1,sd)
r.sse.plus <- r.sse.m + r.sse.sd
r.sse.min <- r.sse.m - r.sse.sd

# Plot differeince between actual SSE mean SSE from 250 randomized datasets - 1st: Log scale, 2nd: Normal scale 

xrange <- range(1:n.lev)
yrange <- range(log(r.sse.plus),log(r.sse.min))
plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='Log of SSE - Random SSE', main='Cluster Solustions against (Log of SSE - Random SSE)')
lines(log(r.sse.m), type="b", col='blue')
lines(log(r.sse.plus), type='l', col='red')
lines(log(r.sse.min), type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), lty=1)

xrange <- range(1:n.lev)
yrange <- range(r.sse.plus,r.sse.min)
plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='SSE - Random SSE', main='Cluster Solutions against (SSE - Random SSE)')
lines(r.sse.m, type="b", col='blue')
lines(r.sse.plus, type='l', col='red')
lines(r.sse.min, type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), lty=1)

# Ask for user input - Select the appropriate number of clusters
#choose.clust <- function(){readline("What clustering solution would you like to use? ")} 
#clust.level <- as.integer(choose.clust())
clust.level <- 5

# Apply K-means cluster solutions - append clusters to CSV file
fit <- kmeans(kdata, clust.level)
aggregate(kdata, by=list(fit$cluster), FUN=mean)
clust.out <- fit$cluster
kclust <- as.matrix(clust.out)
kclust.out <- cbind(kclust, SPDdef)
write.table(kclust.out, file="kmeans_out.csv", sep=",")

# Display Principal Components plot of data with clusters identified

clusplot(kdata, fit$cluster, shade=F, labels=2, lines=0, color=T, lty=4, cex.txt = .3, main='Principal Components plot showing K-means clusters')

```

#### run k-means with midfield players
```{r}
km2 <- kmeans(SPDmid,centers=3)

for (i in 1:5){
  print(paste("Players in Cluster",i))
  print(rowNames[km1$cluster==i])
  print (" ")
}

kdata <- SPDmid
n.lev <- 8

# Calculate the within groups sum of squared error (SSE) for the number of cluster solutions selected by the user
wss <- rnorm(10)
while (prod(wss==sort(wss,decreasing=T))==0) {
  wss <- (nrow(kdata)-1)*sum(apply(kdata,2,var))
  for (i in 2:n.lev) wss[i] <- sum(kmeans(kdata, centers=i)$withinss)}

# Calculate the within groups SSE for 250 randomized data sets (based on the original input data)
k.rand <- function(x){
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

rand.mat <- matrix(0,n.lev,250)

k.1 <- function(x) { 
  for (i in 1:3) {
    r.mat <- as.matrix(suppressWarnings(k.rand(kdata)))
    rand.mat[,i] <- r.mat}
  return(rand.mat)
}

# Same function as above for data with < 3 column variables
k.2.rand <- function(x){
  rand.mat <- matrix(0,n.lev,250)
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

k.2 <- function(x){
  for (i in 1:250) {
    r.1 <- k.2.rand(kdata)
    rand.mat[,i] <- r.1}
  return(rand.mat)
}

# Determine if the data data table has > or < 3 variables and call appropriate function above
if (dim(kdata)[2] == 2) { rand.mat <- k.2(kdata) } else { rand.mat <- k.1(kdata) }

# Plot within groups SSE against all tested cluster solutions for actual and randomized data - 1st: Log scale, 2nd: Normal scale

xrange <- range(1:n.lev)
yrange <- range(log(rand.mat),log(wss))
plot(xrange,yrange, type='n', xlab='Cluster Solution', ylab='Log of Within Group SSE', main='Cluster Solutions against Log of SSE')
for (i in 1:250) lines(log(rand.mat[,i]),type='l',col='red')
lines(log(wss), type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

yrange <- range(rand.mat,wss)
plot(xrange,yrange, type='n', xlab="Cluster Solution", ylab="Within Groups SSE", main="Cluster Solutions against SSE")
for (i in 1:250) lines(rand.mat[,i],type='l',col='red')
lines(1:n.lev, wss, type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

# Calculate the mean and standard deviation of difference between SSE of actual data and SSE of 250 randomized datasets
r.sse <- matrix(0,dim(rand.mat)[1],dim(rand.mat)[2])
wss.1 <- as.matrix(wss)
for (i in 1:dim(r.sse)[2]) {
  r.temp <- abs(rand.mat[,i]-wss.1[,1])
  r.sse[,i] <- r.temp}
r.sse.m <- apply(r.sse,1,mean)
r.sse.sd <- apply(r.sse,1,sd)
r.sse.plus <- r.sse.m + r.sse.sd
r.sse.min <- r.sse.m - r.sse.sd

# Plot differeince between actual SSE mean SSE from 250 randomized datasets - 1st: Log scale, 2nd: Normal scale 

xrange <- range(1:n.lev)
yrange <- range(log(r.sse.plus),log(r.sse.min))
plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='Log of SSE - Random SSE', main='Cluster Solustions against (Log of SSE - Random SSE)')
lines(log(r.sse.m), type="b", col='blue')
lines(log(r.sse.plus), type='l', col='red')
lines(log(r.sse.min), type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), lty=1)

xrange <- range(1:n.lev)
yrange <- range(r.sse.plus,r.sse.min)
plot(xrange,yrange, type='n',xlab='Cluster Solution', ylab='SSE - Random SSE', main='Cluster Solutions against (SSE - Random SSE)')
lines(r.sse.m, type="b", col='blue')
lines(r.sse.plus, type='l', col='red')
lines(r.sse.min, type='l', col='red')
legend('topright',c('SSE - random SSE', 'SD of SSE-random SSE'), col=c('blue', 'red'), lty=1)

# Ask for user input - Select the appropriate number of clusters
#choose.clust <- function(){readline("What clustering solution would you like to use? ")} 
#clust.level <- as.integer(choose.clust())
clust.level <- 5

# Apply K-means cluster solutions - append clusters to CSV file
fit <- kmeans(kdata, clust.level)
aggregate(kdata, by=list(fit$cluster), FUN=mean)
clust.out <- fit$cluster
kclust <- as.matrix(clust.out)
kclust.out <- cbind(kclust, SPDmid)
write.table(kclust.out, file="kmeans_out.csv", sep=",")

# Display Principal Components plot of data with clusters identified

clusplot(kdata, fit$cluster, shade=F, labels=2, lines=0, color=T, lty=4, cex.txt = .3, main='Principal Components plot showing K-means clusters')

```

```{R}
distance <- get_dist(SPDmid)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```
[insert analysis here]

*MANOVA*

#isolate european and latin american subgroups, and make new categorical variable
```{r}
PDeurlat <- PD[ which(PD$Nationality == "Russia" | PD$Nationality == "Germany" | PD$Nationality == "England" | PD$Nationality == "France" | PD$Nationality == "Italy" | PD$Nationality == "Spain" | PD$Nationality == "Ukraine" | PD$Nationality == "Poland" | PD$Nationality == "Romania" | PD$Nationality == "Netherlands" | PD$Nationality == "Argentina" | PD$Nationality == "Bolivia" | PD$Nationality == "Brazil" | PD$Nationality == "Chile" | PD$Nationality == "Colombia" | PD$Nationality == "Ecuador" | PD$Nationality == "Paraguay" | PD$Nationality == "Peru" | PD$Nationality == "Uruguay" | PD$Nationality == "Venezuela"), ]

Region <- ifelse(PD$Nationality == "Russia" | PD$Nationality == "Germany" | PD$Nationality == "England" | PD$Nationality == "France" | PD$Nationality == "Italy" | PD$Nationality == "Spain" | PD$Nationality == "Ukraine" | PD$Nationality == "Poland" | PD$Nationality == "Romania" | PD$Nationality == "Netherlands","Europe","LatinAmerica")

PD$Region <- Region

```

#make interaction plots
```{r}

interaction.plot(PD$Region,PD$InternationalReputation,PD$HeadingAccuracy, lwd=3,col=c("red","blue","black","green","purple"),trace.label = "International Reputation",xlab="Region",ylab="Mean of Heading Accuracy",main="Interaction Plot for Heading Accuracy", legend=FALSE)
legend("topleft", c("Poor","Low","Medium","Fair","Good"),bty="n",lty=2,lwd=3,col=c("red","blue","black","green","purple"), title="International Reputation",inset = 0, cex=.8,y.intersp
=.7, horiz = TRUE)

interaction.plot(PD$Region,PD$InternationalReputation,PD$Agility,lwd=3,col=c("red","blue","black","green","purple"),trace.label="International Reputation",xlab="Region",ylab="Mean of Agility",main="Interaction Plot for Agility",legend=FALSE)
legend("topleft", c("Poor","Low","Medium","Fair","Good"),bty="n",lty=2,lwd=3,col=c("red","blue","black","green","purple"), title="International Reputationt",inset = 0, cex=.8,y.intersp
=.7, horiz = TRUE)

interaction.plot(PD$Region,PD$InternationalReputation,PD$Stamina, lwd=3,col=c("red","blue","black","green","purple"),trace.label = "International Reputation",xlab="Region",ylab="Mean of Stamina",main="Interaction Plot for Stamina", legend=FALSE)
legend("topleft", c("Poor","Low","Medium","Fair","Good"),bty="n",lty=2,lwd=3,col=c("red","blue","black","green","purple"), title="International Reputation",inset = 0, cex=.8,y.intersp
=.7, horiz = TRUE)

interaction.plot(PD$Region,PD$InternationalReputation,PD$Jumping, lwd=3,col=c("red","blue","black","green","purple"),trace.label = "International Reputation",xlab="Region",ylab="Mean of Jumping",main="Interaction Plot for Jumping", legend=FALSE)
legend("topleft", c("Poor","Low","Medium","Fair","Good"),bty="n",lty=2,lwd=3,col=c("red","blue","black","green","purple"), title="International Reputation",inset = 0, cex=.8,y.intersp
=.7, horiz = TRUE)

interaction.plot(PD$Region,PD$InternationalReputation,PD$Balance, lwd=3,col=c("red","blue","black","green","purple"),trace.label = "International Reputation",xlab="Region",ylab="Mean of Balance",main="Interaction Plot for Balance", legend=FALSE)
legend("topleft", c("Poor","Low","Medium","Fair","Good"),bty="n",lty=2,lwd=3,col=c("red","blue","black","green","purple"), title="International Reputation",inset = 0, cex=.8,y.intersp
=.7, horiz = TRUE)

interaction.plot(PD$Region,PD$InternationalReputation,PD$SprintSpeed, lwd=3,col=c("red","blue","black","green","purple"),trace.label = "International Reputation",xlab="Region",ylab="Mean of Sprint Speed",main="Interaction Plot for Sprint Speed", legend=FALSE)
legend("topleft", c("Poor","Low","Medium","Fair","Good"),bty="n",lty=2,lwd=3,col=c("red","blue","black","green","purple"), title="International Reputation",inset = 0, cex=.8,y.intersp
=.7, horiz = TRUE)

interaction.plot(PD$Region,PD$InternationalReputation,PD$Acceleration, lwd=3,col=c("red","blue","black","green","purple"),trace.label = "International Reputation",xlab="Region",ylab="Mean of Acceleration",main="Interaction Plot for Acceleration", legend=FALSE)
legend("topleft", c("Poor","Low","Medium","Fair","Good"),bty="n",lty=2,lwd=3,col=c("red","blue","black","green","purple"), title="International Reputation",inset = 0, cex=.8,y.intersp
=.7, horiz = TRUE)
```

```
[insert analysis here]

#Conclusions and Discussion

#Points for further analysis
